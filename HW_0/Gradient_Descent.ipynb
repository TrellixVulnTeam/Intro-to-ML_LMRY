{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 0: Gradient Descent Algorithm\n",
    "\n",
    "**Kimsour Thach** *800793249*\n",
    "\n",
    "*ECGR 5060*\n",
    "\n",
    "\n",
    "\n",
    "## Introduction \n",
    "\n",
    "The following code will perform the steps needed to perform a Gradient Descent on a ML model.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# First import all needed libraries\n",
    "\n",
    "import numpy as np     # Arrays\n",
    "import pandas as pd    # Data structures\n",
    "import matplotlib.pyplot as plt # Plotting\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem 1\n",
    "\n",
    "Develop a code that run linear regression with gradient decent algorithm for each of the explanatory variables in isolation. In this case, you assume that in each iteration, only one explanatory variable (either X1, or X2, or X3) is explaining the output. Basically, you need to do three different training, one per each explanatory variable. For the learning rate, explore different values between 0.1 and 0.01 (your choice). Initialize your parameters to zero (theta to zero).\n",
    "\n",
    "* Report the linear model you found for each explanatory variable.\n",
    "* Plot the final regression model and loss over the iteration per each explanatory variable.\n",
    "\n",
    "* **Which explanatory variable has the lower loss (cost) for explaining the output (Y)?**\n",
    "\n",
    "* **Based on your training observations, describe the impact of the different learning rates on the final loss and number of training iteration.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Read in CSV file\n",
    "# The CSV file should be contained in the same directory\n",
    "\n",
    "df = pd.read_csv('D3.csv')\n",
    "\n",
    "#Let's check out the first 5 rows of our dataset\n",
    "\n",
    "print(df.head())\n",
    "print(len(df))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "        0.0  3.4399999999999999  0.44000000000000039  4.387545011044053\n",
      "0  0.040404            0.134949             0.888485           2.679650\n",
      "1  0.080808            0.829899             1.336970           2.968490\n",
      "2  0.121212            1.524848             1.785455           3.254065\n",
      "3  0.161616            2.219798             2.233939           3.536375\n",
      "4  0.202020            2.914747             2.682424           3.815420\n",
      "99\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Define explanatory variables (First 3 columns)\n",
    "\n",
    "x1 = df.values[:,0]\n",
    "x2 = df.values[:,1]\n",
    "x3 = df.values[:,2]\n",
    "y = df.values[:,3]\n",
    "m = len(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training 1\n",
    "\n",
    "**Cost/loss function**\n",
    "\n",
    "h = theta_0 + theta_1*x_1\n",
    "\n",
    "x_0 is always one, create a vector full of 1 to the length of the sample points.\n",
    "\n",
    "Calculated Model = [x0 , x1] * [theta_0 theta_1] "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# x1\n",
    "plt.scatter(x1,y,color='red',marker='+')\n",
    "\n",
    "# Create a vector of ones\n",
    "\n",
    "x_0=np.ones((m,1))\n",
    "\n",
    "# Stack the 2 vectors together\n",
    "\n",
    "x_1=x1.reshape(m,1)\n",
    "\n",
    "X=np.hstack((x_0,x_1))\n",
    "\n",
    "theta=np.zeros(2)\n",
    "\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "def compute_cost(X,y,theta):\n",
    "\n",
    "    predictions=X.dot(theta)\n",
    "    errors=np.subtract(predictions,y)\n",
    "    sqrErrors=np.square(errors)\n",
    "    J=1/(2*m)*np.sum(sqrErrors)\n",
    "    \n",
    "    return J\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fdfefc3ed90>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Compute cost for linear regression.  \n",
    "Input Parameters  \n",
    "----------------  \n",
    "X : 2D array where each row represent the training example and each column represent       \n",
    "m= number of training examples      \n",
    "n= number of features (including X_0 column of ones)  \n",
    "y : 1D array of labels/target value for each traing example. dimension(1 x m)  \n",
    "theta : 1D array of fitting parameters or weights. Dimension (1 x n)  Output Parameters  \n",
    "-----------------  \n",
    "J : Scalar value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training 2\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x2\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training 3\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x3\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem 2\n",
    "\n",
    "This time, run linear regression with gradient descent algorithm using all three explanatory variables. For the learning rate, explore different values between 0.1 and 0.01 (your choice). Initialize your parameters (theta to zero).\n",
    "\n",
    "* Report the final linear model you found the best. \n",
    "* Plot loss over the iteration.\n",
    "* Based on your training observations, describe the impact of the different learning rates on the final loss and number of training iteration.\n",
    "* Predict the value of y for new (X1, X2, X3) values (1, 1, 1), for (2, 0, 4), and for (3, 2, 1)\n",
    "\n",
    "Several parameters, organize parameters into a matrix...."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}